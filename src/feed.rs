use crate::{db, llm_functions};
use anyhow::Result;
use axum::{
    extract::Path,
    http::StatusCode,
    response::{IntoResponse, Response},
};
use chrono::Utc;
use dashmap::DashMap;
use feed_rs::{model::Entry, parser};
use futures::future::join_all;
use reqwest::{Client, header::CONTENT_TYPE};
use rss::{Channel, ChannelBuilder, Guid, ItemBuilder};
use serde::{Deserialize, Serialize};
use std::sync::LazyLock;
use tracing::{info, warn};

static HTTP_CLIENT: LazyLock<Client> = LazyLock::new(Client::new);
static FEEDS_OUTPUT: LazyLock<DashMap<String, Channel>> = LazyLock::new(DashMap::new);
static FEEDS_INPUT: LazyLock<Vec<WebsiteFeed>> = LazyLock::new(|| {
    vec![WebsiteFeed {
        id: "verge".to_string(),
        url: "https://www.theverge.com/rss/index.xml".to_string(),
        clean_title: true,
        summarise_articles: true,
    }]
});

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WebsiteFeed {
    pub id: String,
    pub url: String,
    pub clean_title: bool,
    pub summarise_articles: bool,
}

/// Ensures the `SQLite` database is initialised and cached feeds are hydrated.
pub async fn init_storage() -> Result<()> {
    db::init().await?;

    let rows = db::load_all_channels().await?;
    FEEDS_OUTPUT.clear();
    for (id, channel) in rows {
        FEEDS_OUTPUT.insert(id, channel);
    }

    Ok(())
}

/// Refreshes all feeds concurrently
pub async fn refresh_all_feeds() -> Result<()> {
    let refresh_tasks = FEEDS_INPUT.iter().map(|feed| async {
        if let Err(err) = refresh_feed(feed).await {
            warn!(feed_id = %feed.id, "Failed to refresh feed: {err:#}");
        }
    });

    // Await all refresh tasks to complete.
    join_all(refresh_tasks).await;

    Ok(())
}

/// Refreshes a single feed and updates the cache.
async fn refresh_feed(feed: &WebsiteFeed) -> Result<()> {
    info!(feed_id = %feed.id, url = %feed.url, "Refreshing feed");

    // Fetch and parse the remote feed
    let content = HTTP_CLIENT.get(&feed.url).send().await?.bytes().await?;
    let fetched = parser::parse(content.as_ref())?;

    // Process all entries concurrently
    let items = join_all(
        fetched
            .entries
            .into_iter()
            .map(|entry| build_item(feed, entry)),
    )
    .await;

    let title = fetched
        .title
        .map_or_else(|| feed.url.clone(), |text| text.content);
    let link = fetched
        .links
        .first()
        .map_or_else(|| feed.url.clone(), |link| link.href.clone());

    // Build the new RSS channel from the processed items
    let channel = ChannelBuilder::default()
        .title(title)
        .link(link)
        .description(fetched.description.map_or_else(
            || "Summarised feed generated by Quivrs".to_string(),
            |text| text.content,
        ))
        .last_build_date(
            fetched
                .updated
                .map_or_else(|| Utc::now().to_rfc2822(), |dt| dt.to_rfc2822()),
        )
        .items(items)
        .build();

    db::save_channel(&feed.id, &channel).await?;
    FEEDS_OUTPUT.insert(feed.id.clone(), channel);
    Ok(())
}

/// Builds a single RSS item, checking for existing items and optionally summarizing content.
async fn build_item(feed: &WebsiteFeed, entry: Entry) -> rss::Item {
    // If the item already exists in our output cache, clone and return it to avoid reprocessing.
    if let Some(channel) = FEEDS_OUTPUT.get(&feed.id)
        && let Some(existing) = channel
            .items()
            .iter()
            .find(|item| item.guid().is_some_and(|guid| guid.value() == entry.id))
    {
        return existing.clone();
    }

    let link = entry
        .links
        .first()
        .map_or_else(|| feed.url.clone(), |l| l.href.clone());
    info!(link = %link, "Loading webpage");

    // Try to fetch the full article content; fall back to the feed's summary.
    let description_html = async { HTTP_CLIENT.get(&link).send().await?.text().await }
        .await
        .ok()
        .unwrap_or_else(|| entry.content.and_then(|c| c.body).unwrap_or_default());

    let original_description =
        html2text::from_read(description_html.as_bytes(), 120).unwrap_or_default();
    let original_title = entry.title.map_or_else(|| feed.url.clone(), |t| t.content);

    let (title, description) = if feed.clean_title || feed.summarise_articles {
        match llm_functions::summarise_article(&original_title, &original_description).await {
            Ok(output) => (output.title, output.content),
            Err(err) => {
                warn!(
                    feed_id = %feed.id,
                    entry_id = %entry.id,
                    "Summarization failed: {err:#}"
                );
                (original_title, original_description)
            }
        }
    } else {
        (original_title, original_description)
    };

    let mut builder = ItemBuilder::default();
    builder
        .title(title)
        .link(link)
        .description(description)
        .guid(Guid {
            value: entry.id,
            permalink: true,
        });
    if let Some(published) = entry.published.map(|date| date.to_rfc2822()) {
        builder.pub_date(published);
    }
    builder.build()
}

/// Axum handler to serve the generated feed.
pub async fn summarised_feed_handler(Path(id): Path<String>) -> Response {
    FEEDS_OUTPUT.get(&id).map_or_else(
        || StatusCode::NOT_FOUND.into_response(),
        |channel| {
            (
                [(CONTENT_TYPE, "application/rss+xml; charset=utf-8")],
                channel.to_string(),
            )
                .into_response()
        },
    )
}
